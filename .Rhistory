result <- tibble::tibble(
estimator = character(), pred.error = numeric(), sparsity = integer(), f1.score.nz = numeric(),
f1.score.fr = numeric(), id = integer(), n = integer(), m = integer(), p = integer(),
s = integer(), s.r = integer(), rho = numeric(), snr = numeric()
)
# Generate data
data <- gendata(par)
x <- data$x
y <- data$y
y.val <- data$y.val
cluster <- data$cluster
b <- data$b
xb <- data$xb
# Fit glmmsel
fit <- glmmsel::glmmsel(x, y, cluster, intercept = FALSE)
best.lambda <- fit$lambda[which.min(colSums((y.val - predict(fit, x, cluster)) ^ 2))]
b.hat <- coef(fit, lambda = best.lambda)
y.hat <- predict(fit, x, cluster, lambda = best.lambda)
result <- evalmod(result, 'glmmsel', b.hat, b, y.hat, xb, par)
# Fit L0Learn
fit <- L0Learn::L0Learn.fit(x, y, intercept = FALSE)
best.lambda <- fit$lambda[[1]][which.min(colSums((y.val - as.matrix(predict(fit, x))) ^ 2))]
b.hat <- unname(t(as.numeric(coef(fit, lambda = best.lambda))))
y.hat <- as.numeric(predict(fit, x, lambda = best.lambda))
result <- evalmod(result, 'L0Learn', b.hat, b, y.hat, xb, par)
# Fit glmnet
fit <- glmnet::glmnet(x, y, intercept = FALSE, relax = TRUE)
best.lambda <- fit$lambda[which.min(colSums((y.val - predict(fit, x, gamma = 0)) ^ 2))]
b.hat <- unname(t(as.numeric(coef(fit, s = best.lambda, gamma = 0)[- 1])))
y.hat <- as.numeric(predict(fit, x, s = best.lambda, gamma = 0))
result <- evalmod(result, 'glmnet', b.hat, b, y.hat, xb, par)
# # Fit pysr3
# x_ <- cbind(x, cluster, 1)[order(cluster), ]
# y_ <- y[order(cluster)]
# columns_labels <- c(rep('fixed+random', par[['p']]), 'group', 'variance')
# L0LmeModelSR3 <- reticulate::import('pysr3.lme.models')$L0LmeModelSR3
# n_k <- min(par[['p']], 30) + 1
# mse <- numeric(n_k)
# b.hat <- vector('list', n_k)
# y.hat <- vector('list', n_k)
# initial_parameters <- NULL
# for (k in seq_len(n_k) - 1) {
#   model <- L0LmeModelSR3(nnz_tbeta = as.integer(k), practical = TRUE)
#   model$fit(x_, y_, columns_labels = columns_labels, initial_parameters = initial_parameters)
#   # initial_parameters <- reticulate::dict(beta = model$coef_$beta, gamma = model$coef_$gamma)
#   b.hat[[k + 1]] <- model$coef_$per_group_coefficients[, 2:(par[['p']] + 1), drop = FALSE]
#   y.hat[[k + 1]] <- model$predict(x_, columns_labels = columns_labels)[order(order(cluster))]
#   mse[k + 1] <- mean((y.val - y.hat[[k + 1]]) ^ 2)
# }
# b.hat <- b.hat[[which.min(mse)]]
# y.hat <- y.hat[[which.min(mse)]]
# result <- evalmod(result, 'pysr3', b.hat, b, y.hat, xb, par)
# # Fit rpql
# fit <- rpql::rpqlseq(y, x, list(cluster = x), id = list(cluster = cluster),
#                      lambda = exp(seq(log(1), log(1e-2), l = 50)), intercept = FALSE,
#                      pen.type = 'mcp')
# b.hat <- sweep(fit$best.fits[[1]]$ranef$cluster, 2, fit$best.fits[[1]]$fixef, '+')
# y.hat <- rowSums(x * b.hat[cluster, ])
# result <- evalmod(result, 'rpql', b.hat, b, y.hat, xb, par)
}
# Construct scenarios
simulations <- tidyr::expand_grid(
id = 1:10, # Simulation run ID
n = round(exp(seq(log(10), log(1000), length.out = 7))), # Number of samples
m = 5, # Number of clusters
p = 1000, # Number of predictors
s = 5, # Number of nonzero predictors
s.r = 3, # Number of fixed+random predictors
rho = 0.5, # Correlation coefficient
snr = 1#exp(seq(log(0.1), log(10), length.out = 7)) # Signal-to-noise ratio
)
# Run simulations
cl <- parabar::start_backend(parallel::detectCores() - 1)
parabar::export(cl, variables = c('gendata', 'evalmod'))
result <- parabar::par_apply(cl, simulations, 1, runsim)
result <- dplyr::bind_rows(result)
parabar::stop_backend(cl)
# Write results
readr::write_csv(result, '../Experiments/Results/gaussian.csv')
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/gaussian-2.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/binomial-1.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/binomial-2.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/binomial-1.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/binomial-2.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/gaussian-1.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/gaussian-2.R", echo=TRUE)
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/binomial-1.R", echo=TRUE)
# Set theme
ggplot2::theme_set(ggplot2::theme_bw())
# Load results
result <- readr::read_csv('../Experiments/Results/binomial.csv') |>
tidyr::pivot_longer(c(pred.error, f1.score.fr, sparsity)) |>
dplyr::mutate(name = factor(name, c('pred.error', 'f1.score.fr', 'sparsity'),
c('Prediction error', 'F1 score', 'Sparsity'))) |>
dplyr::mutate(estimator = factor(estimator, c('glmmsel', 'L0Learn', 'glmnet')))
# Plot results
result |>
ggplot2::ggplot(ggplot2::aes(n, value, color = estimator)) +
ggplot2::geom_point(stat = 'summary', fun = mean) +
ggplot2::geom_line(stat = 'summary', fun = mean) +
ggplot2::geom_errorbar(stat = 'summary', fun.data = ggplot2::mean_se) +
ggplot2::facet_wrap(. ~ name, scales = 'free') +
ggplot2::scale_x_log10() +
ggplot2::geom_hline(
ggplot2::aes(yintercept = value),
tibble::tibble(name = unique(result$name), value = c(NA, NA, result$s[1])),
linetype = 'dashed'
) +
ggplot2::xlab('Signal-to-noise ratio') +
ggplot2::ylab(ggplot2::element_blank()) +
ggplot2::theme(legend.title = ggplot2::element_blank()) +
ggplot2::theme(legend.position = 'bottom') +
ggsci::scale_color_npg() +
ggplot2::coord_cartesian(ylim = c(0, 1))
# Set theme
ggplot2::theme_set(ggplot2::theme_bw())
# Load results
result <- readr::read_csv('../Experiments/Results/binomial.csv') |>
tidyr::pivot_longer(c(pred.error, f1.score.fr, sparsity)) |>
dplyr::mutate(name = factor(name, c('pred.error', 'f1.score.fr', 'sparsity'),
c('Prediction error', 'F1 score', 'Sparsity'))) |>
dplyr::mutate(estimator = factor(estimator, c('glmmsel', 'L0Learn', 'glmnet')))
# Plot results
result |>
ggplot2::ggplot(ggplot2::aes(n, value, color = estimator)) +
ggplot2::geom_point(stat = 'summary', fun = mean) +
ggplot2::geom_line(stat = 'summary', fun = mean) +
ggplot2::geom_errorbar(stat = 'summary', fun.data = ggplot2::mean_se) +
ggplot2::facet_wrap(. ~ name, scales = 'free') +
ggplot2::scale_x_log10() +
ggplot2::geom_hline(
ggplot2::aes(yintercept = value),
tibble::tibble(name = unique(result$name), value = c(NA, NA, result$s[1])),
linetype = 'dashed'
) +
ggplot2::xlab('Signal-to-noise ratio') +
ggplot2::ylab(ggplot2::element_blank()) +
ggplot2::theme(legend.title = ggplot2::element_blank()) +
ggplot2::theme(legend.position = 'bottom') +
ggsci::scale_color_npg() +
ggplot2::coord_cartesian(ylim = c(0, 1))
cv.loss <- \(xb, y) {
pi <- pmax(1e-5, pmin(1 - 1e-5, 1 / (1 + exp(- xb))))
- mean(y * log(pi) + (1 - y) * log(1 - pi))
}
#==================================================================================================#
# Function to generate data
#==================================================================================================#
gendata <- \(par) {
# Save scenario parameters
n <- par[['n']]
# m <- par[['m']]
m <- ceiling(n / 50)
p <- par[['p']]
s <- par[['s']]
s.r <- par[['s.r']]
rho <- par[['rho']]
snr <- par[['snr']]
# Generate predictors
Sigma <- rho ^ t(sapply(1:p, \(i, j) abs(i - j), 1:p))
x <- MASS::mvrnorm(n, rep(0, p), Sigma)
# Construct coefficients
nz.fixed.id <- sample(p, s)
beta <- numeric(p)
beta[nz.fixed.id] <- 1
u <- matrix(0, m, p)
nz.random.id <- sample(nz.fixed.id, s.r)
u[, nz.random.id] <- matrix(rnorm(m * s.r), m, s.r)
b <- sweep(u, 2, beta, '+')
# Generate response
# cluster <- rep(1, n)
# cluster <- sample(1:m, n, replace = TRUE)
cluster <- rep_len(1:m, n)
xb <- rowSums(x * b[cluster, ])
y <- rbinom(n, 1, plogis(xb * snr))
while (sum(y == 1) < 2 || sum(y == 0) < 2) y <- rbinom(n, 1, plogis(xb * snr))
y.val <- rbinom(n, 1, plogis(xb * snr))
# y.te <- rbinom(n, 1, plogis(xb * snr))
# y <- rnorm(n, xb, sd(xb) / sqrt(snr))
# y.val <- rnorm(n, xb, sd(xb) / sqrt(snr))
# y.te <- rnorm(n, xb, sd(xb) / sqrt(snr))
# y <- pmax(sign(y), 0)
# y.val <- pmax(sign(y.val), 0)
# y.te <- pmax(sign(y.te), 0)
# Return generated data
list(x = x, y = y, y.val = y.val, xb = xb, cluster = cluster, b = b)
}
#==================================================================================================#
# Function to evaluate a fitted model
#==================================================================================================#
evalmod <- \(result, estimator, b.hat, b, y.hat, xb, par, pql) {
# Save scenario parameters
id <- par[['id']]
n <- par[['n']]
m <- par[['m']]
p <- par[['p']]
s <- par[['s']]
s.r <- par[['s.r']]
rho <- par[['rho']]
snr <- par[['snr']]
# Compress into vector
b.norm <- apply(b, 2, \(x) sum(x ^ 2))
b.hat.norm <- apply(b.hat, 2, \(x) sum(x ^ 2))
b.sd <- apply(b, 2, \(x) mean((mean(x) - x) ^ 2))
b.hat.sd <- apply(b.hat, 2, \(x) mean((mean(x) - x) ^ 2))
# Compute prediction error
pred.error <- sum((xb - y.hat) ^ 2) / sum(xb ^ 2)
# pi <- pmax(1e-5, pmin(1 - 1e-5, 1 / (1 + exp(- y.hat))))
# pred.error <- - mean(y.te * log(pi) + (1 - y.te) * log(1 - pi))
# pred.error.null <- - mean(y.te * log(0.5) + (1 - y.te) * log(0.5))
# pred.error <- pred.error / pred.error.null
# Compute sparsity level
sparsity <- sum(b.hat.norm != 0)
# Compute F1 score of zero vs. nonzero
tp <- sum(b.norm != 0 & b.hat.norm != 0)
fp <- sum(b.norm == 0 & b.hat.norm != 0)
fn <- sum(b.norm != 0 & b.hat.norm == 0)
f1.score.nz <- 2 * tp / (2 * tp + fp + fn)
# Compute F1 score of zero vs. fixed vs. fixed+random
tp.f <- sum((b.norm != 0 & b.sd == 0) & (b.hat.norm != 0 & b.hat.sd == 0))
fp.f <- sum(b.norm == 0 & (b.hat.norm != 0 & b.hat.sd == 0))
fn.f <- sum((b.norm != 0 & b.sd == 0) & b.hat.norm == 0)
tp.r <- sum(b.sd != 0 & b.hat.sd != 0)
fp.r <- sum(b.sd == 0 & b.hat.sd != 0)
fn.r <- sum(b.sd != 0 & b.hat.sd == 0)
tp <- tp.f + tp.r
fp <- fp.f + fp.r
fn <- fn.f + fn.r
f1.score.fr <- 2 * tp / (2 * tp + fp + fn)
# Update results
tibble::add_row(result, estimator, pred.error, sparsity, f1.score.nz, f1.score.fr, id, n, m, p,
s, s.r, rho, snr, pql)
}
#==================================================================================================#
# Function to run a single simulation
#==================================================================================================#
runsim <- \(par) {
# Set RNG seed
seed <- digest::digest2int(digest::digest(par))
set.seed(seed)
# Create space to store results
result <- tibble::tibble(
estimator = character(), pred.error = numeric(), sparsity = integer(), f1.score.nz = numeric(),
f1.score.fr = numeric(), id = integer(), n = integer(), m = integer(), p = integer(),
s = integer(), s.r = integer(), rho = numeric(), snr = numeric(), pql = numeric()
)
# Generate data
data <- gendata(par)
x <- data$x
y <- data$y
y.val <- data$y.val
cluster <- data$cluster
b <- data$b
xb <- data$xb
# Fit glmmsel
fit <- glmmsel::glmmsel(x, y, cluster, intercept = FALSE, family = 'binomial')
best.lambda <- fit$lambda[which.min(apply(predict(fit, x, cluster), 2, cv.loss, y.val))]
b.hat <- coef(fit, lambda = best.lambda)
y.hat <- predict(fit, x, cluster, lambda = best.lambda)
result <- evalmod(result, 'glmmsel', b.hat, b, y.hat, xb, par, max(fit$pql))
# Fit L0Learn
fit <- L0Learn::L0Learn.fit(x, y, intercept = FALSE, loss = 'Logistic')
pi.hat <-  apply(as.matrix(predict(fit, x)), 2, \(x) pmax(1e-5, pmin(1 - 1e-5, x)))
xb.hat <- log(pi.hat / (1 - pi.hat))
best.lambda <- fit$lambda[[1]][which.min(apply(xb.hat, 2, cv.loss, y.val))]
b.hat <- unname(t(as.numeric(coef(fit, lambda = best.lambda))))
xb.hat <- pmax(1e-5, pmin(1 - 1e-5, predict(fit, x, lambda = best.lambda)))
y.hat <- log(xb.hat / (1 - xb.hat))
result <- evalmod(result, 'L0Learn', b.hat, b, y.hat, xb, par, 0)
# Fit glmnet
fit <- glmnet::glmnet(x, y, intercept = FALSE, relax = TRUE, family = 'binomial')
best.lambda <- fit$lambda[which.min(apply(predict(fit, x, gamma = 0), 2, cv.loss, y.val))]
b.hat <- unname(t(as.numeric(coef(fit, s = best.lambda, gamma = 0)[- 1])))
y.hat <- as.numeric(predict(fit, x, s = best.lambda, gamma = 0))
result <- evalmod(result, 'glmnet', b.hat, b, y.hat, xb, par, 0)
# # Fit pysr3
# x_ <- cbind(x, cluster, 1)[order(cluster), ]
# y_ <- y[order(cluster)]
# columns_labels <- c(rep('fixed+random', par[['p']]), 'group', 'variance')
# L0LmeModelSR3 <- reticulate::import('pysr3.lme.models')$L0LmeModelSR3
# n_k <- min(par[['p']], 30) + 1
# mse <- numeric(n_k)
# b.hat <- vector('list', n_k)
# y.hat <- vector('list', n_k)
# initial_parameters <- NULL
# for (k in seq_len(n_k) - 1) {
#   model <- L0LmeModelSR3(nnz_tbeta = as.integer(k), practical = TRUE)
#   model$fit(x_, y_, columns_labels = columns_labels, initial_parameters = initial_parameters)
#   # initial_parameters <- reticulate::dict(beta = model$coef_$beta, gamma = model$coef_$gamma)
#   b.hat[[k + 1]] <- model$coef_$per_group_coefficients[, 2:(par[['p']] + 1), drop = FALSE]
#   y.hat[[k + 1]] <- model$predict(x_, columns_labels = columns_labels)[order(order(cluster))]
#   mse[k + 1] <- mean((y.val - y.hat[[k + 1]]) ^ 2)
# }
# b.hat <- b.hat[[which.min(mse)]]
# y.hat <- y.hat[[which.min(mse)]]
# result <- evalmod(result, 'pysr3', b.hat, b, y.hat, xb, par)
result
}
# Construct scenarios
simulations <- tidyr::expand_grid(
id = 1:100, # Simulation run ID
n = round(exp(seq(log(10), log(1000), length.out = 7))), # Number of samples
m = 5, # Number of clusters
p = 1000, # Number of predictors
s = 5, # Number of nonzero predictors
s.r = 3, # Number of fixed+random predictors
rho = 0.5, # Correlation coefficient
snr = 1 # Signal-to-noise ratio
)
# Run simulations
system.time({
cl <- parabar::start_backend(parallel::detectCores() - 1)
parabar::export(cl, variables = c('gendata', 'evalmod', 'cv.loss'))
result <- parabar::par_apply(cl, simulations, 1, runsim)
result <- dplyr::bind_rows(result)
parabar::stop_backend(cl)
})
# Write results
readr::write_csv(result, '../Experiments/Results/binomial.csv')
result |>
dplyr::filter(estimator == 'glmmsel') |>
dplyr::group_by(n) |>
dplyr::summarise(max(pql))
# Set theme
ggplot2::theme_set(ggplot2::theme_bw())
# Load results
result <- readr::read_csv('../Experiments/Results/binomial.csv') |>
tidyr::pivot_longer(c(pred.error, f1.score.fr, sparsity)) |>
dplyr::mutate(name = factor(name, c('pred.error', 'f1.score.fr', 'sparsity'),
c('Prediction error', 'F1 score', 'Sparsity'))) |>
dplyr::mutate(estimator = factor(estimator, c('glmmsel', 'L0Learn', 'glmnet')))
# Plot results
result |>
ggplot2::ggplot(ggplot2::aes(n, value, color = estimator)) +
ggplot2::geom_point(stat = 'summary', fun = mean) +
ggplot2::geom_line(stat = 'summary', fun = mean) +
ggplot2::geom_errorbar(stat = 'summary', fun.data = ggplot2::mean_se) +
ggplot2::facet_wrap(. ~ name, scales = 'free') +
ggplot2::scale_x_log10() +
ggplot2::geom_hline(
ggplot2::aes(yintercept = value),
tibble::tibble(name = unique(result$name), value = c(NA, NA, result$s[1])),
linetype = 'dashed'
) +
ggplot2::xlab('Signal-to-noise ratio') +
ggplot2::ylab(ggplot2::element_blank()) +
ggplot2::theme(legend.title = ggplot2::element_blank()) +
ggplot2::theme(legend.position = 'bottom') +
ggsci::scale_color_npg() +
ggplot2::coord_cartesian(ylim = c(0, 1))
# Load and prepare data
riboflavin <- readr::read_csv('Data/riboflavingrouped.csv')[, - 1] |>
t() |>
unname()
y <- riboflavin[, 1]
x <- riboflavin[, - 1]
cluster <- readr::read_csv('Data/riboflavingrouped_structure.csv', col_names = FALSE)$X1
cluster <- gsub('[^0-9]', '', cluster) |>
as.numeric()
ncol(x)
nrow(x)
setwd("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments")
setwd("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments")
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/riboflavin-1.R", echo=TRUE)
# Load data
readr::read_csv(paste0('Results/riboflavin.csv')) |>
# Compute point estimates and standard errors
dplyr::group_by(estimator) |>
dplyr::summarise(
pred.error.se = sd(pred.error) / sqrt(dplyr::n()),
pred.error = mean(pred.error),
sparsity.se = sd(sparsity) / sqrt(dplyr::n()),
sparsity = mean(sparsity),
sparsity.r.se = sd(sparsity.r) / sqrt(dplyr::n()),
sparsity.r = mean(sparsity.r)
) |>
# Format statistics to certain number of decimal places
dplyr::mutate(
pred.error.se = format(round(pred.error.se, 3), nsmall = 3, trim = T),
pred.error = format(round(pred.error, 3), nsmall = 3, trim = T),
sparsity.se = format(round(sparsity.se, 1), nsmall = 1, trim = T),
sparsity = format(round(sparsity, 1), nsmall = 1, trim = T),
sparsity.r.se = format(round(sparsity.r.se, 1), nsmall = 1, trim = T),
sparsity.r = format(round(sparsity.r, 1), nsmall = 1, trim = T)
) |>
# Place standard errors in brackets after point estimates
dplyr::transmute(
estimator = estimator,
pred.error = paste0(pred.error, ' (', pred.error.se, ')'),
sparsity = paste0(sparsity, ' (', sparsity.se, ')'),
sparsity.r = paste0(sparsity.r, ' (', sparsity.r.se, ')')
) |>
# Create table
gt::gt() |>
# Set column headings
gt::cols_label(
estimator = '',
pred.error = 'Prediction error',
sparsity = 'Total',
sparsity.r = 'Random'
) |>
gt::tab_spanner('Sparsity', 3:4)
#==================================================================================================#
# Function to generate data
#==================================================================================================#
gendata <- \(par) {
# Save scenario parameters
test.perc <- par[['test.perc']]
# Load and prepare data
data('basal', package = 'glmmPen')
x <- unname(basal$X)
x <- x + apply(x, 2, \(x) min(unique(x)))
y <- basal$y
cluster <- as.integer(basal$group)
# Add interactions
x <- as.data.frame(x)
x <- model.matrix(as.formula(paste('~ 0 + (', paste(names(x), collapse = ' + '), ') ^ 2')), x)
x <- unname(x)
# Split data into training and testing sets
test.ind <- sample(nrow(x), round(nrow(x) * test.perc))
x.train <- x[- test.ind, , drop = FALSE]
x.test <- x[test.ind, , drop = FALSE]
y.train <- y[- test.ind]
y.test <- y[test.ind]
cluster.train <- cluster[- test.ind]
cluster.test <- cluster[test.ind]
# Return generated data
list(x.train = x.train, x.test = x.test, y.train = y.train, y.test = y.test,
cluster.train = cluster.train, cluster.test = cluster.test)
}
#==================================================================================================#
# Function to evaluate a fitted model
#==================================================================================================#
evalmod <- \(result, estimator, b.hat, y.hat, y.test, y.train, par) {
# Save scenario parameters
id <- par[['id']]
test.perc <- par[['test.perc']]
# Compress into vector
b.hat <- b.hat[, - 1, drop = FALSE]
b.hat.norm <- apply(b.hat, 2, \(x) sum(x ^ 2))
b.hat.sd <- apply(b.hat, 2, \(x) mean((mean(x) - x) ^ 2))
# Compute prediction error
pi <- pmax(1e-5, pmin(1 - 1e-5, 1 / (1 + exp(- y.hat))))
pred.error <- - mean(y.test * log(pi) + (1 - y.test) * log(1 - pi))
pred.error.null <- - mean(y.test * log(mean(y.train)) + (1 - y.test) * log(1 - mean(y.train)))
pred.error <- pred.error / pred.error.null
# Compute sparsity level
sparsity <- sum(b.hat.norm != 0)
sparsity.r <- sum(b.hat.sd != 0)
# Update results
tibble::add_row(result, estimator, pred.error, sparsity, sparsity.r, id, test.perc)
}
#==================================================================================================#
# Function to run a single simulation
#==================================================================================================#
runsim <- \(par) {
# Set RNG seed
seed <- digest::digest2int(digest::digest(par))
set.seed(seed)
# Create space to store results
result <- tibble::tibble(
estimator = character(), pred.error = numeric(), sparsity = integer(), sparsity.r = integer(),
id = integer(), test.perc = numeric()
)
# Generate data
data <- gendata(par)
x.train <- data$x.train
x.test <- data$x.test
y.train <- data$y.train
y.test <- data$y.test
cluster.train <- data$cluster.train
cluster.test <- data$cluster.test
# Fit glmmsel
fit <- glmmsel::cv.glmmsel(x.train, y.train, cluster.train, family = 'binomial')
b.hat <- coef(fit)
y.hat <- predict(fit, x.test, cluster.test)
result <- evalmod(result, 'glmmsel', b.hat, y.hat, y.test, y.train, par)
# Fit grpsel
fit <- grpsel::cv.grpsel(x.train, y.train, loss = 'logistic')
b.hat <- t(coef(fit))
y.hat <- predict(fit, x.test)
result <- evalmod(result, 'grpsel', b.hat, y.hat, y.test, y.train, par)
# # Fit L0Learn
# fit <- L0Learn::L0Learn.cvfit(x.train, y.train, loss = 'Logistic')
# best.lambda <- fit$fit$lambda[[1]][which.min(fit$cvMeans[[1]])]
# b.hat <- unname(t(as.numeric(coef(fit, lambda = best.lambda))))
# y.hat <- as.numeric(predict(fit, x.test, lambda = best.lambda))
# result <- evalmod(result, 'L0Learn', b.hat, y.hat, y.test, y.train, par)
# Fit glmnet
fit <- glmnet::cv.glmnet(x.train, y.train, relax = TRUE, gamma = 0, family = 'binomial')
b.hat <- unname(t(as.numeric(coef(fit, s = 'lambda.min'))))
y.hat <- as.numeric(predict(fit, x.test, s = 'lambda.min'))
result <- evalmod(result, 'glmnet', b.hat, y.hat, y.test, y.train, par)
result
}
# Construct scenarios
simulations <- tidyr::expand_grid(
id = 1:30, # Simulation run ID
test.perc = 0.1 # Hold-out size as a percentage of total number of observations
)
# Run simulations
system.time({
cl <- parabar::start_backend(parallel::detectCores() - 1)
parabar::export(cl, variables = c('gendata', 'evalmod'))
result <- parabar::par_apply(cl, simulations, 1, runsim)
result <- dplyr::bind_rows(result)
parabar::stop_backend(cl)
})
# Write results
readr::write_csv(result, '../Experiments/Results/basal.csv')
source("~/Dropbox/Professional/Research/Papers/Sparse linear mixed models/Experiments/basal-2.R", echo=TRUE)
install.packages('lme4')
